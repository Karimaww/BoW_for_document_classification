{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reconnaissance du locuteur : Chirac - Mitterrand"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paramètres du notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "corpus_language = \"french\"\n",
    "folder_path = \"./datasets/AFDpresidentutf8/\"\n",
    "corpus_basename = \"corpus.tache1\"\n",
    "corpus_base_path = folder_path + corpus_basename\n",
    "\n",
    "file_name_train = corpus_base_path + \".learn.utf8\"\n",
    "file_name_test = corpus_base_path + \".test.utf8\"\n",
    "file_name_output = corpus_base_path + \".test.pred.utf8\"\n",
    "\n",
    "results_save_path = \"../Reports/report_project/results/locuteur/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "training = {\"small\": 30, \"medium\": 200, \"large\": 2000}\n",
    "study_config = \"medium\"\n",
    "final_config = \"large\"\n",
    "\n",
    "# minimum number of uppercase characters in a word to be considered as uppercase\n",
    "# (e.g. \"upper\" + \"HELLO\" = \"upperhello\") if uppercase_markers is True\n",
    "# during preprocessing\n",
    "min_upper_char = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sklearn_working_memory = 24 * 1024"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Imports bibliothèques Python\n",
    "import codecs\n",
    "import re\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Imports bibliothèques externes\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "sklearn.set_config(working_memory=sklearn_working_memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mathis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mathis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Téléchargement des ressources nécessaires à NLTK\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Paramètres matplotlib\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"pgf\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"text.usetex\": True,\n",
    "        \"pgf.rcfonts\": False,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Chargement des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def load_pres(path):\n",
    "    texts_list, labels_list = [], []\n",
    "    regex_pattern = r\"<[0-9]*:[0-9]*(:(?P<locuteur>C|M))?> (?P<paroles>.*)\"\n",
    "    compiled_regex_pattern = re.compile(pattern=regex_pattern)\n",
    "    s = codecs.open(path, \"r\", \"utf-8\")\n",
    "    for text in s.readlines():\n",
    "        regex_search = re.search(pattern=compiled_regex_pattern, string=text)\n",
    "        if regex_search.group(\"locuteur\") is not None:\n",
    "            if regex_search.group(\"locuteur\") == \"C\":\n",
    "                labels_list.append(\"Chirac\")\n",
    "            elif regex_search.group(\"locuteur\") == \"M\":\n",
    "                labels_list.append(\"Mitterrand\")\n",
    "            else:\n",
    "                raise ValueError(\"Locuteur inconnu\")\n",
    "        if regex_search.group(\"paroles\") is not None:\n",
    "            texts_list.append(regex_search.group(\"paroles\"))\n",
    "    return texts_list, labels_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "texts, labels = load_pres(file_name_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Données non équilibrées"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57413 exemples\n",
      "Exemples Mitterrand : 7523 (13.10%)\n",
      "Exemples Chirac : 49890 (86.90%)\n"
     ]
    }
   ],
   "source": [
    "n = len(labels)\n",
    "print(n, \"exemples\")\n",
    "for president in [\"Mitterrand\", \"Chirac\"]:\n",
    "    count = labels.count(president)\n",
    "    percentage = count / n * 100\n",
    "    print(f\"Exemples {president} : {count} ({percentage:.2f}%)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Traitement des données"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Prétraitement"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# A pattern to match continuous uppercase substrings,\n",
    "# including accented uppercase characters\n",
    "# Only matches substrings of min_upper_char or more characters\n",
    "upper = r\"\\b(?:[A-ZÀ-ÖØ-öø-ÿĀ-ſƀ-ɏḀ-ỿꜢ-ꞇﬀ-ﬆＡ-Ｚ]{{{},}})\\b\"\n",
    "upper_case_pattern = upper.format(min_upper_char)\n",
    "upper_case_pattern_compiled = re.compile(pattern=upper_case_pattern)\n",
    "upper_replacement = lambda m: \"upper\" + m.group(0).lower()\n",
    "\n",
    "\n",
    "def custom_preprocessor(\n",
    "    text, remove_punctuation=True, remove_numbers=True, uppercase_markers=False\n",
    "):\n",
    "    if uppercase_markers:\n",
    "        text = re.sub(\n",
    "            pattern=upper_case_pattern_compiled, repl=upper_replacement, string=text\n",
    "        )\n",
    "\n",
    "    if remove_punctuation:\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    if remove_numbers:\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.digits))\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def custom_tokenizer(text, stemmer=None, lemmatizer=None):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    tokens = word_tokenize(text, language=corpus_language)\n",
    "\n",
    "    assert (\n",
    "        stemmer is None or lemmatizer is None\n",
    "    ), \"Only one of stemmer or lemmatizer can be used.\"\n",
    "\n",
    "    if stemmer is not None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    if lemmatizer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Transformation occurrences -> TF-IDF\n",
    "\n",
    "Sera systématiquement appliqué si un modèle utilisant LSA comme réduction de dimension est utilisé"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "param_tfidftransformer = {\n",
    "    \"transformer\": [TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)],\n",
    "    \"transformer__norm\": [\"l1\", \"l2\", None],\n",
    "}\n",
    "\n",
    "param_grid_tfidf_transformer = [\n",
    "    {\n",
    "        \"transformer\": [None],\n",
    "    },\n",
    "    param_tfidftransformer,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## d) Réduction de la dimension\n",
    "\n",
    "Ne sera pas employé sur le classifieur Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, norm, randint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "param_grid_dim_reduction = [\n",
    "    {\n",
    "        \"dim_reduction\": [None],\n",
    "    },\n",
    "    {\n",
    "        \"dim_reduction\": [TruncatedSVD()],\n",
    "        \"dim_reduction__n_components\": randint(10, 30),\n",
    "        \"dim_reduction__algorithm\": [\"randomized\", \"arpack\"],\n",
    "    },\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Modèles étudiés"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Classifieur Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# fit_prior=True car les données sont stratifiées\n",
    "# ne sera pas forcément le cas en production, cf. 9)\n",
    "multinomial_classifier = MultinomialNB(fit_prior=True)\n",
    "nb_regul_distrib = uniform(loc=0.8, scale=0.2)\n",
    "param_grid_nb = {\n",
    "    \"classifier\": [multinomial_classifier],\n",
    "    \"classifier__alpha\": nb_regul_distrib,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Classifieur SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc_classifier = LinearSVC(class_weight=\"balanced\", dual=False)\n",
    "svm_regul_distrib = norm(loc=1.0, scale=0.2)\n",
    "param_grid_svm = {\n",
    "    \"classifier\": [linear_svc_classifier],\n",
    "    \"classifier__C\": svm_regul_distrib,\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c) Classifieur régression logistique"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_classifier = LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\")\n",
    "log_reg_regul_distrib = norm(loc=1.0, scale=0.2)\n",
    "param_grid_log_reg = {\n",
    "    \"classifier\": [log_reg_classifier],\n",
    "    \"classifier__C\": log_reg_regul_distrib,\n",
    "    \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grille de paramètres des classifieurs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "param_grid_classifier = [\n",
    "    param_grid_nb,\n",
    "    param_grid_svm,\n",
    "    param_grid_log_reg,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Metrique de selection de modèles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import average_precision_score, balanced_accuracy_score\n",
    "\n",
    "# average precision score\n",
    "# scorer = make_scorer(average_precision_score)\n",
    "# average recall score\n",
    "# scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# area under the ROC curve\n",
    "# scorer = make_scorer(score_func=roc_auc_score, needs_proba=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On choisit beta=1.5 car on veut être sûr d'avoir un recall plus élevé pour Mitterrand"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "scorer = make_scorer(fbeta_score, beta=1.5, pos_label=\"Mitterrand\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5) Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a) Assemblage du pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=\n",
    "    [\n",
    "        (\"vectorizer\", CountVectorizer(token_pattern=None)),\n",
    "        (\"transformer\", None),\n",
    "        (\"dim_reduction\", None),\n",
    "        (\"scaler\", None),\n",
    "        (\"classifier\", None),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b) Grille de recherche"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"vectorizer__binary\": [True, False],\n",
    "        \"vectorizer__lowercase\": [True, False],\n",
    "        \"vectorizer__max_features\": [None, 2000, 5000, 10000],\n",
    "        \"vectorizer__strip_accents\": [\"ascii\", \"unicode\", None],\n",
    "        \"vectorizer__preprocessor\": [\n",
    "            lambda text: custom_preprocessor(\n",
    "                text,\n",
    "                remove_punctuation=remove_punct,\n",
    "                remove_numbers=remove_nums,\n",
    "                uppercase_markers=uppercase_marks,\n",
    "            )\n",
    "            for remove_punct in [True, False]\n",
    "            for remove_nums in [True, False]\n",
    "            for uppercase_marks in [True, False]\n",
    "        ],\n",
    "        \"vectorizer__tokenizer\": [\n",
    "            lambda text: custom_tokenizer(text, stemmer=stem, lemmatizer=None)\n",
    "            for stem in [None, SnowballStemmer(language=corpus_language)]\n",
    "        ],\n",
    "        # min_df and max_df instead of stopwords list\n",
    "        \"vectorizer__min_df\": uniform(loc=0.0, scale=0.02),\n",
    "        \"vectorizer__max_df\": uniform(loc=0.5, scale=0.45),\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)],\n",
    "        **dim_reduction_params,\n",
    "        **(\n",
    "            # tfidf transformer is recommended when dimensionality reduction is used\n",
    "            transformer_params\n",
    "            if dim_reduction_params[\"dim_reduction\"] is None\n",
    "            else param_tfidftransformer\n",
    "        ),\n",
    "        # scaler useless with multinomial naive bayes classifier\n",
    "        # mean = False to avoid transforming the sparse matrix into a dense one\n",
    "        \"scaler\": [\n",
    "            None\n",
    "            if isinstance(classifier_params[\"classifier\"][0], MultinomialNB)\n",
    "            else StandardScaler(with_mean=False)\n",
    "        ],\n",
    "        **classifier_params,\n",
    "    }\n",
    "    for classifier_params in param_grid_classifier\n",
    "    for transformer_params in param_grid_tfidf_transformer\n",
    "    for dim_reduction_params in (\n",
    "        [param_grid_dim_reduction[0]]\n",
    "        if isinstance(classifier_params[\"classifier\"][0], MultinomialNB)\n",
    "        else param_grid_dim_reduction[1:]\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6) Recherche du meilleur modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split train/test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42, shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stratified K-Fold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recherche : Halving Random Search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# noinspection PyUnresolvedReferences\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    factor=2,\n",
    "    n_candidates=training[study_config],\n",
    "    min_resources=4000,\n",
    "    max_resources=\"auto\",\n",
    "    aggressive_elimination=True,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "fitted_search = search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7) Comparaison des modèles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sauvegarde des résultats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_results(model_search, name: str):\n",
    "    results = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(model_search.cv_results_[\"params\"]),\n",
    "            pd.DataFrame(\n",
    "                model_search.cv_results_[\"mean_test_score\"], columns=[\"score\"]\n",
    "            ),\n",
    "            pd.DataFrame(\n",
    "                model_search.cv_results_[\"std_test_score\"], columns=[\"std_score\"]\n",
    "            ),\n",
    "            pd.DataFrame(model_search.cv_results_[\"rank_test_score\"], columns=[\"rank\"]),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    results.sort_values(\"rank\", axis=0, ascending=True, inplace=True, kind=\"quicksort\")\n",
    "    results.to_csv(name + \".csv\")\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "study_results_file_name = \"Study_HalvingRandomSearchCV_results\"\n",
    "study_results_save_path = results_save_path + study_results_file_name\n",
    "study_results = save_results(fitted_search, study_results_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Affichage des résultats de la cross-validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Get the cv results\n",
    "cv_results = fitted_search.cv_results_\n",
    "\n",
    "# Extract mean and std of test scores\n",
    "mean_scores = fitted_search.cv_results_[\"mean_test_score\"]\n",
    "std_scores = fitted_search.cv_results_[\"std_test_score\"]\n",
    "\n",
    "top_indices = np.argsort(mean_scores)[::-1][: training[study_config]]\n",
    "top_mean_scores = mean_scores[top_indices]\n",
    "top_std_scores = std_scores[top_indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(training[study_config]) + 1\n",
    "plt.plot(x, top_mean_scores, label=\"Mean score\")\n",
    "plt.fill_between(\n",
    "    x,\n",
    "    top_mean_scores - top_std_scores,\n",
    "    top_mean_scores + top_std_scores,\n",
    "    alpha=0.2,\n",
    "    label=\"±1 std\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"models\")\n",
    "plt.ylabel(\"Validation score\")\n",
    "plt.title(\n",
    "    \"Mean validation scores and standard deviations of top {} models\".format(\n",
    "        training[study_config]\n",
    "    )\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "model_rank_global_file_name = \"model_rank_global.pgf\"\n",
    "model_rank_save_path = results_save_path + model_rank_global_file_name\n",
    "plt.savefig(model_rank_save_path)\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Number of top models to plot\n",
    "n_top_models = min(80, training[study_config] // 3)\n",
    "\n",
    "# Classifier colors\n",
    "classifier_colors = {\n",
    "    \"MultinomialNB\": \"blue\",\n",
    "    \"LinearSVC\": \"green\",\n",
    "    \"LogisticRegression\": \"red\",\n",
    "}\n",
    "\n",
    "# Get the indices of the top n models sorted by mean_test_score\n",
    "top_n_indices = np.argsort(cv_results[\"mean_test_score\"])[-n_top_models:][::-1]\n",
    "\n",
    "# Plot the mean validated score and standard deviation for the top n models\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, index in enumerate(top_n_indices):\n",
    "    mean_score = cv_results[\"mean_test_score\"][index]\n",
    "    std_score = cv_results[\"std_test_score\"][index]\n",
    "    classifier_type = cv_results[\"param_classifier\"][index].__class__.__name__\n",
    "    color = classifier_colors.get(classifier_type, \"black\")\n",
    "    ax.errorbar(\n",
    "        i + 1,\n",
    "        mean_score,\n",
    "        yerr=std_score,\n",
    "        fmt=\"\",\n",
    "        color=color,\n",
    "        label=f\"{classifier_type}\",\n",
    "    )\n",
    "\n",
    "handles, ax_labels = ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(ax_labels, handles))\n",
    "ax.legend(by_label.values(), by_label.keys(), loc=\"upper right\")\n",
    "\n",
    "ax.set_xlabel(\"models\")\n",
    "ax.set_ylabel(\"Validation score\")\n",
    "ax.set_title(\n",
    "    \"Mean validation scores and standard deviations for the top {} models\".format(\n",
    "        n_top_models\n",
    "    )\n",
    ")\n",
    "\n",
    "model_rank_classifier_file_name = \"model_rank_classifier.pgf\"\n",
    "model_rank_classifier_save_path = results_save_path + model_rank_classifier_file_name\n",
    "plt.savefig(model_rank_classifier_save_path)\n",
    "\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8) Meilleur pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meilleur score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pipeline: Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(max_df=0.7056999405180515,\n",
      "                                 min_df=0.0023172701772555125,\n",
      "                                 ngram_range=(1, 2),\n",
      "                                 preprocessor=<function <listcomp>.<listcomp>.<lambda> at 0x136b59e40>,\n",
      "                                 token_pattern=None,\n",
      "                                 tokenizer=<function <listcomp>.<listcomp>.<lambda> at 0x136b5a2a0>)),\n",
      "                ('transformer', TfidfTransformer(norm=None, sublinear_tf=True)),\n",
      "                ('dim_reduction', None), ('scaler', None),\n",
      "                ('classifier', MultinomialNB(alpha=0.833728345921864))],\n",
      "         verbose=True)\n",
      "Best classifier: MultinomialNB(alpha=0.833728345921864)\n"
     ]
    }
   ],
   "source": [
    "best_pipeline = fitted_search.best_estimator_\n",
    "best_classifier = best_pipeline.named_steps[\"classifier\"]\n",
    "\n",
    "print(\"Best pipeline:\", best_pipeline)\n",
    "print(\"Best classifier:\", best_classifier)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation finale du meilleur modèle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "y_pred = best_pipeline.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Chirac      0.947     0.792     0.862      9978\n",
      "  Mitterrand      0.339     0.708     0.458      1505\n",
      "\n",
      "    accuracy                          0.781     11483\n",
      "   macro avg      0.643     0.750     0.660     11483\n",
      "weighted avg      0.867     0.781     0.809     11483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matrice de confusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x163339a10>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    normalize=\"all\",\n",
    "    cmap=\"winter\",\n",
    "    values_format=\".3f\",\n",
    "    text_kw={\"fontsize\": 18, \"color\": \"red\", \"fontweight\": \"bold\"},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Courbe ROC / Precision-Recall / DET"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(\n",
    "    best_pipeline, X_test, y_test, name=\"ROC curve\", drop_intermediate=True\n",
    ")\n",
    "\n",
    "plt.savefig(results_save_path + \"roc_curve.pgf\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(\n",
    "    best_pipeline, X_test, y_test, name=\"Precision-Recall curve\"\n",
    ")\n",
    "\n",
    "plt.savefig(results_save_path + \"precision_recall_curve.pgf\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from sklearn.metrics import DetCurveDisplay\n",
    "\n",
    "DetCurveDisplay.from_estimator(\n",
    "    best_pipeline, X_test, y_test, name=\"DET curve\", pos_label=\"Mitterrand\"\n",
    ")\n",
    "\n",
    "plt.savefig(results_save_path + \"det_curve.pgf\")\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9) Application sur les données sans labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nouvelle recherche avec cross-validation sur l'ensemble des données\n",
    "Le modèle final ne sera pas évalué sur des données de test.\n",
    "L'évaluation se fera lors de la soumission des prédictions sur le serveur de test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "final_search = fitted_search.set_params(\n",
    "    **{\"n_candidates\": training[final_config],\n",
    "       \"factor\": 1.5,\n",
    "       \"refit\": True\n",
    "       }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_fitted_search = final_search.fit(texts, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "final_results_file_name = \"Final_HalvingRandomSearchCV_results\"\n",
    "final_results_save_path = results_save_path + study_results_file_name\n",
    "final_results = save_results(final_fitted_search, final_results_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "final_model = final_fitted_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# we may have more Mitterrand than Chirac to detect when the model is in production\n",
    "# we don't know if the data are identically distributed in the test set\n",
    "# Therefore fit_prior=False for Naive Bayes would be better in this case, cf. 3)a) above\n",
    "if isinstance(final_model.named_steps[\"classifier\"], MultinomialNB):\n",
    "    final_model.named_steps[\"classifier\"].fit_prior = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prédiction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "texts, _ = load_pres(file_name_test)\n",
    "y_pred_test = final_model.predict(texts)\n",
    "\n",
    "# Sauvegarde des prédictions\n",
    "with open(file_name_output, \"w\") as f:\n",
    "    for president in y_pred_test:\n",
    "        f.write(president[0] + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
